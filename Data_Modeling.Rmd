---
title: "Modeling for Animal Shelter Data"
output: html_notebook
---

```{r}
library(tidyverse)
library(ggplot2)
library(lubridate)
library(DataExplorer)
library(randomForest)
library(mlbench)
library(caret)
```


```{r}
cad_model <- read.csv("Data/Cleaned_Data_Frame.csv") %>%
             select(-X)
```


Prep the data for modeling
```{r}
cad_model <- cad_model %>%
       select(-AnimalID, -DateTime, -MonthYear, -Found.Location, -Breed, -breed_new, -Color)
```
```{r}
glimpse(cad_model)
```
```{r}
cad_model <- cad_model  %>% 
       dummify(maxcat = 11L, select = c("S_Organ_Status_In", "Sex", "S_Organ_Status_Out", "AnimalType", "Intake.Type", "Intake.Condition", "breed_group"))
```
```{r}
glimpse(cad_model)
```

##Handle Dummified Columns
After having dummified the columns, we rename and remove certain columns that would be duplicative or are confusingly named.
```{r}
cad_model <- cad_model %>%
       select(-AnimalType_Cat) %>%
       dplyr::rename(Is_Dog = AnimalType_Dog)
```


We will be modeling on OutcomeType variable
```{r data_split}
seed <- 333
inTraining <- createDataPartition(cad_model$OutcomeType, p=0.6, list=FALSE)
training.set <- cad_model[inTraining,]
Totalvalidation.set <- cad_model[-inTraining,]

# This will create another partition of the 40% of the data, so 20%-testing and 20%-validation

inValidation <- createDataPartition(Totalvalidation.set$OutcomeType, p=0.5, list=FALSE)
testing.set <- Totalvalidation.set[inValidation,]
validation.set <- Totalvalidation.set[-inValidation,]
```
```{r}
library("foreign")
library("nnet")
test <- multinom(OutcomeType ~ ., data = training.set)
```
```{r}
summary(test)
```
```{r}
predict(test, testing.set, type = "prob")
```
```{r}
error_rate <- table(predict(test, testing.set), testing.set$OutcomeType)
```
```{r}
print(error_rate)
```

#Random Forest Model
```{r}
rf_fit <- train(OutcomeType ~ ., 
                data = training.set, 
                method = "rf", 
                metric = "Accuracy",
                preProcess = c("center", "scale"),
                tuneLength = 10)

rf_pred <- predict(rf_fit, newdata=testing.set)

confusionMatrix(data=rf_pred, testing.set$OutcomeType)
```

```{r}
#Find the best mtry value to use in my model
x <- training.set %>%
    select(-OutcomeType, -Color, -Breed, -DateTime, -MonthYear)
y <- training.set %>%
    select(OutcomeType) %>%
    .$OutcomeType
# Algorithm Tune (tuneRF)
set.seed(seed)
bestmtry <- tuneRF(x, y, stepFactor=1.5, improve=1e-5, ntree=500)
print(bestmtry)
```

```{r}
metric <- "Accuracy"

#Find the correct number of trees (ntree) to use in my model
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid")
tunegrid <- expand.grid(.mtry=c(sqrt(ncol(training.set))))
modellist <- list()
for (ntree in c(1000, 1500, 2000, 2500)) {
	set.seed(seed)
	fit <- train(OutcomeType ~ ., data=training.set, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control, ntree=ntree)
	key <- toString(ntree)
	modellist[[key]] <- fit
}
# compare results
results <- resamples(modellist)
summary(results)
dotplot(results)
```
