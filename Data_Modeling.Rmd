---
title: "Modeling for Animal Shelter Data"
output: html_notebook
---

```{r}
library(tidyverse)
library(ggplot2)
library(lubridate)
library(randomForest)
library(caret)
library(viridis)
```


```{r}
cad_model <- read.csv("Data/Cleaned_Data_Frame.csv") %>%
             select(-X)
```

Prep the data for modeling
```{r}
cad_model <- cad_model %>%
       select(-AnimalID, -DateTime, -MonthYear, -Found.Location, -Breed, -breed_new, -Color) %>% 
       dummify(maxcat = 11L, select = c("S_Organ_Status_In", "Sex", "S_Organ_Status_Out", "AnimalType", "Intake.Type", "Intake.Condition", "breed_group"))
```
##Handle Dummified Columns
After having dummified the columns, we rename and remove certain columns that would be duplicative or are confusingly named.
```{r}
cad_model <- cad_model %>%
       select(-AnimalType_Cat) %>%
       dplyr::rename(Is_Dog = AnimalType_Dog)
```


We will be modeling on OutcomeType variable
```{r data_split}
seed <- 333
inTraining <- createDataPartition(cad_model$OutcomeType, p=0.6, list=FALSE)
training.set <- cad_model[inTraining,]
Totalvalidation.set <- cad_model[-inTraining,]

# This will create another partition of the 40% of the data, so 20%-testing and 20%-validation

inValidation <- createDataPartition(Totalvalidation.set$OutcomeType, p=0.5, list=FALSE)
testing.set <- Totalvalidation.set[inValidation,]
validation.set <- Totalvalidation.set[-inValidation,]
```

#Random Forest Model
```{r}
rf_fit <- train(OutcomeType ~ ., 
                data = training.set, 
                method = "rf", 
                metric = "Accuracy",
                preProcess = c("center", "scale"),
                tuneLength = 10)

rf_pred <- predict(rf_fit, newdata=testing.set)

confusionMatrix(data=rf_pred, testing.set$OutcomeType)
```

Accuracy after initial run: 0.6922

#                     Class: Adoption Class: Died Class: Euthanasia Class: Return_to_owner Class: Transfer
Sensitivity                   0.8556   0.0263158           0.32012                 0.6473          0.5997
Specificity                   0.7213   1.0000000           0.99259                 0.9006          0.9179
Pos Pred Value                0.6743   1.0000000           0.72414                 0.6122          0.7887
Neg Pred Value                0.8810   0.9935360           0.96004                 0.9133          0.8178
Prevalence                    0.4028   0.0066376           0.05729                 0.1951          0.3382
Detection Rate                0.3446   0.0001747           0.01834                 0.1263          0.2028
Detection Prevalence          0.5111   0.0001747           0.02533                 0.2063          0.2571
Balanced Accuracy             0.7884   0.5131579           0.65636                 0.7739          0.7588

```{r save model parameters}
save(rf_fit, file="rf_initial_fit.rda")
```

Find best mtry values
```{r}
#Find the best mtry value to use in my model
x <- training.set %>%
    select(-OutcomeType)
y <- training.set %>%
    select(OutcomeType) %>%
    .$OutcomeType
# Algorithm Tune (tuneRF)
set.seed(seed)
bestmtry <- tuneRF(x, y, stepFactor=1.5, improve=1e-5, ntree=500)
print(bestmtry)
```

Find optimized number of trees
```{r}
metric <- "Accuracy"

#Find the correct number of trees (ntree) to use in my model
control <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid")
tunegrid <- expand.grid(.mtry=c(sqrt(ncol(training.set))))
modellist <- list()
for (ntree in c(1000, 1500, 2000, 2500)) {
	set.seed(seed)
	fit <- train(OutcomeType ~ ., data=training.set, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control, ntree=ntree)
	key <- toString(ntree)
	modellist[[key]] <- fit
}
# compare results
results <- resamples(modellist)
summary(results)
dotplot(results)
```

Find optimized predictor variables
```{r}
#Find best predictor variables
rf_pv <- randomForest(Churn_Yes ~ ., data=telco_data, ntree=1000, keep.forest=FALSE, importance=TRUE)
importance(rf_pv) # relative importance of predictors (highest <-> most important)
varImpPlot(rf_pv) # plot results
```

